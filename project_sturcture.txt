--> create database
--> create table?
--> create ingestion script for the first time
--> first use the script to manually download
    --> using gemini one
--> use dbt to create layers
--> figure out if we want to re-create the table everytime or not
    --> if not we use ephemeral
--> create airflow
--> use airflow to run everything

crypto_data_pipeline/
├── airflow/
│   ├── dags/
│   │   └── crypto_etl_dag.py
│   └── scripts/
│       └── extract_coingecko_data.py
├── dbt/
│   └── crypto_analytics/
│       ├── models/
│       │   ├── staging/
│       │   │   └── stg_coingecko_prices.sql
│       │   └── marts/
│       │       └── dim_coins.sql
│       │       └── fact_daily_prices.sql
│       ├── profiles.yml # Symlink or copy of ~/.dbt/profiles.yml (for self-containment)
│       ├── dbt_project.yml
│       ├── tests/
│       ├── macros/
│       ├── seeds/
│       └── README.md
├── .gitignore
├── README.md
└── requirements.txt

--------------------------------------------------------------------------------------------
NEW STRUCTURE
crypto_data_pipeline/
├── .gitignore
├── README.md
├── requirements.txt
├── crypto_data_env/          # Your Python virtual environment
│   └── bin/ (or Scripts/)
│   └── lib/
├── airflow_configs/          # This will be your AIRFLOW_HOME
│   ├── airflow.cfg           # Auto-generated by Airflow
│   ├── airflow.db            # Auto-generated by Airflow (for SQLite)
│   ├── logs/                 # Auto-generated by Airflow
│   └── dags/                 # Where your DAGs go
│       └── crypto_etl_dag.py
│       └── scripts/          # For your helper scripts, like extract_coingecko_data.py
│           └── extract_coingecko_data.py
└── dbt/
    └── crypto_analytics/
        ├── models/
        │   ├── staging/
        │   │   └── stg_coingecko_prices.sql
        │   └── marts/
        │       └── dim_coins.sql
        │       └── fact_daily_prices.sql
        ├── profiles.yml      # Can be a copy/symlink from ~/.dbt/
        ├── dbt_project.yml
        └── ... (tests, macros, etc.)